{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQKUdedwJl86"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "from argparse import ArgumentParser\n",
        "import glob\n",
        "import cv2\n",
        "import re\n",
        "import os, glob, datetime\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "from skimage.transform import rescale\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "from tensorflow.keras.layers import  Input,Conv2D, BatchNormalization,Activation,Subtract, Reshape\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.python.keras.utils import conv_utils\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "#import data_generator as dg\n",
        "import tensorflow.keras.backend as K\n",
        "import skimage\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.io import imread, imsave\n",
        "\n",
        "\n",
        "#########--------------           IMPORTANT NOTE       ----------------###############\n",
        "\n",
        "# Select the path of the 'train_data' and 'save_dir' directories properly to give access to the models, datasets and results\n",
        "\n",
        "# Check whether the training images are normalized or not. Use noise level Sigma accordingly.\n",
        "\n",
        "\n",
        "\n",
        "## Params\n",
        "parser = argparse.ArgumentParser(description='Keras DIVA2D')\n",
        "parser.add_argument('--model', default='DIVA2D', type=str, help='choose a type of model')\n",
        "parser.add_argument('--kernel_size', default=5, type=int, help='kernel size')\n",
        "parser.add_argument('--batch_size', default=128, type=int, help='batch size')\n",
        "parser.add_argument('--train_data', default='./data/training_set', type=str, help='path of train data')\n",
        "parser.add_argument('--sigma', default=15, type=int, help='noise level')\n",
        "parser.add_argument('--epoch', default=100, type=int, help='number of train epoches')\n",
        "parser.add_argument('--lr', default=1e-3, type=float, help='initial learning rate for Adam')\n",
        "parser.add_argument('--save_every', default=1, type=int, help='save model at every x epoches')\n",
        "parser.add_argument('-f', '--file', required=False) \n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "kernel_size = args.kernel_size\n",
        "batch_size = args.batch_size\n",
        "\n",
        "save_dir = os.path.join('./models', args.model+'_sigma_'+ str(args.sigma) )\n",
        "\n",
        "if not os.path.exists(save_dir):\n",
        "     print(\"save_dir\",save_dir)     \n",
        "     os.mkdir(save_dir)\n",
        "\n",
        "\n",
        "\n",
        "##--------------------------------------------------------------------------------------------------------\n",
        "class Hamiltonian_Conv2D(Conv2D):\n",
        "\n",
        "    def __init__(self, filters, kernel_size, kernel_3=None, kernel_4=None, activation=None, **kwargs):\n",
        "\n",
        "        self.rank = 2               # Dimension of the kernel\n",
        "        self.num_filters = filters  # Number of filter in the convolution layer\n",
        "        self.kernel_size = conv_utils.normalize_tuple(kernel_size, self.rank, 'kernel_size')\n",
        "        self.kernel_3 = kernel_3    # Weights from original potential\n",
        "        self.kernel_4 = kernel_4    # Weights from interaction     \n",
        "\n",
        "        super(Hamiltonian_Conv2D, self).__init__(self.num_filters, self.kernel_size, \n",
        "              activation=activation, use_bias=False, **kwargs)\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "        if input_shape[channel_axis] is None:\n",
        "            raise ValueError('The channel dimension of the inputs '\n",
        "                     'should be defined. Found `None`.')\n",
        "\n",
        "        #don't use bias:\n",
        "        self.bias = None\n",
        "\n",
        "        #consider the layer built\n",
        "        self.built = True\n",
        "\n",
        "\n",
        "        # Define nabla operator\n",
        "        weights_1 = tf.constant([[ 2.,-1., 0.],\n",
        "                                 [-1., 4.,-1.],\n",
        "                                 [ 0.,-1., 2.]])\n",
        "        \n",
        "\n",
        "        weights_1 = tf.reshape(weights_1 , [3,3, 1])\n",
        "        weights_1 = tf.repeat(weights_1 , repeats=self.num_filters, axis=2)\n",
        "        #print('kernel shape of weights_1:',weights_1.get_shape())\n",
        "\n",
        "        # Define Weights for h^2/2m  (size should be same as the nabla operator)\n",
        "        weights_2 = self.add_weight(shape=weights_1.get_shape(),\n",
        "                                      initializer= 'Orthogonal',\n",
        "                                      name='kernel_h^2/2m',\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        #print('kernel shape of weights_2:',weights_2.get_shape())\n",
        "\n",
        "        \n",
        "        # Define the Hamiltonian kernel\n",
        "        self.kernel = weights_1*weights_2 + self.kernel_3 + self.kernel_4\n",
        "        #print('self.kernel',self.kernel.get_shape())\n",
        "\n",
        "        self.built = True\n",
        "        super(Hamiltonian_Conv2D, self).build(input_shape)\n",
        "\n",
        "    # Do the 2D convolution using the Hamiltonian kernel\n",
        "    def convolution_op(self, inputs, kernel):\n",
        "        if self.padding == \"causal\":\n",
        "            tf_padding = \"VALID\"  # Causal padding handled in `call`.\n",
        "        elif isinstance(self.padding, str):\n",
        "            tf_padding = self.padding.upper()\n",
        "        else:\n",
        "            tf_padding = self.padding\n",
        "\n",
        "\n",
        "        return tf.nn.convolution(\n",
        "            inputs,\n",
        "            kernel,\n",
        "            strides=list(self.strides),\n",
        "            padding=tf_padding,\n",
        "            dilations=list(self.dilation_rate),\n",
        "            data_format=self._tf_data_format,\n",
        "            name=self.__class__.__name__,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        outputs = self.convolution_op(inputs, self.kernel)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def DIVA2D(depth,filters=64,image_channels=1, kernel_size= args.kernel_size, use_bnorm=True):\n",
        "    layer_count = 0\n",
        "    inpt = Input(shape=(None,None,image_channels),name = 'input'+str(layer_count))\n",
        "    \n",
        "    # Get the initial patches /initial_patches\n",
        "    initial_patches = Conv2D(filters=filters, kernel_size=(kernel_size,kernel_size), strides=(1,1),kernel_initializer='Orthogonal', padding='same',name = 'initial_patches')(inpt)\n",
        "    initial_patches = Activation('relu',name = 'initial_patch_acti')(initial_patches)\n",
        "    #print(initial_patches.get_shape())\n",
        "\n",
        "    # interaction layer\n",
        "    inter = Conv2D(filters=filters, kernel_size=(kernel_size,kernel_size), strides=(1,1),kernel_initializer='Orthogonal', padding='same',name = 'interactions')(initial_patches)\n",
        "    inter = Activation('relu',name = 'interaction_acti'+str(layer_count))(inter)\n",
        "    #print(inter.get_shape())\n",
        "\n",
        "\n",
        "    # Get contributions of the original potential in the Hamiltonian kernel\n",
        "    ori_poten_kernel = tf.keras.layers.MaxPooling2D (pool_size=(21,21), strides=(15,15), padding='same', name = 'ori_poten_ker', data_format=None )(initial_patches)\n",
        "    #print('ori_poten_kernel',ori_poten_kernel.get_shape())\n",
        "\n",
        "    # Get contributions of the interactions in the Hamiltonian kernel\n",
        "    inter_kernel = tf.keras.layers.MaxPooling2D (pool_size=(21,21), strides=(15,15), padding='same', name = 'inter_ker', data_format=None )(inter)\n",
        "    #print('inter_kernel',inter_kernel.get_shape())\n",
        "\n",
        "\n",
        "    # Get projection coefficients of the initial patches on the Hamiltonian kernel\n",
        "    x = Hamiltonian_Conv2D(filters=filters, kernel_size=(kernel_size,kernel_size), kernel_3 = ori_poten_kernel, kernel_4 = inter_kernel, strides=(1,1), activation='relu',\n",
        "                              kernel_initializer='Orthogonal', padding='same', name = 'proj_coef')(initial_patches)      \n",
        "    \n",
        "    #print('coef',x.get_shape())\n",
        "\n",
        "\n",
        "    # Do Thresholding (depth depends on the noise intensity)\n",
        "    for i in range(depth-2):\n",
        "      layer_count += 1\n",
        "      x = Conv2D(filters=filters, kernel_size=(kernel_size,kernel_size), strides=(1,1),kernel_initializer='Orthogonal', padding='same',use_bias = False,name = 'conv'+str(layer_count))(x)\n",
        "\n",
        "      layer_count += 1\n",
        "      x = BatchNormalization(axis=3, momentum=0.1,epsilon=0.0001, name = 'bn'+str(layer_count))(x) \n",
        "        #x = BatchNormalization(axis=3, momentum=0.0,epsilon=0.0001, name = 'bn'+str(layer_count))(x)\n",
        "      \n",
        "      # Thresholding\n",
        "      x = Activation('relu',name = 'Thresholding'+str(layer_count))(x)  \n",
        "\n",
        "    # Inverse projection\n",
        "    x = Conv2D(filters=image_channels, kernel_size=(kernel_size,kernel_size), strides=(1,1), kernel_initializer='Orthogonal',padding='same',use_bias = False,name = 'inv_trans')(x)\n",
        "\n",
        "    x = Subtract(name = 'subtract')([inpt, x])\n",
        "\n",
        "    model = Model(inputs=inpt, outputs=x)\n",
        "    \n",
        "    return model\n",
        "\n",
        "##----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def findLastCheckpoint(save_dir):\n",
        "    file_list = glob.glob(os.path.join(save_dir,'model_*.hdf5'))  # get name list of all .hdf5 files\n",
        "    #file_list = os.listdir(save_dir)\n",
        "    if file_list:\n",
        "        epochs_exist = []\n",
        "        for file_ in file_list:\n",
        "            result = re.findall(\".*model_(.*).hdf5.*\",file_)\n",
        "            #print(result[0])\n",
        "            epochs_exist.append(int(result[0]))\n",
        "        initial_epoch=max(epochs_exist)   \n",
        "    else:\n",
        "        initial_epoch = 0\n",
        "    return initial_epoch\n",
        "\n",
        "\n",
        "def show(x,title=None,cbar=False,figsize=None):\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(x,interpolation='nearest',cmap='gray')\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    if cbar:\n",
        "        plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def log(*args,**kwargs):\n",
        "     print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S:\"),*args,**kwargs)\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    initial_lr = args.lr\n",
        "    if epoch<=20:\n",
        "        lr = initial_lr\n",
        "    elif epoch<=30:\n",
        "        lr = initial_lr/10\n",
        "    elif epoch<=40:\n",
        "        lr = initial_lr/20\n",
        "    else:\n",
        "        lr = initial_lr/20\n",
        "    log('current learning rate is %2.8f' %lr)\n",
        "    return lr\n",
        "\n",
        "def train_datagen(epoch_iter=2000,epoch_num=5,batch_size=128,data_dir=args.train_data):\n",
        "  import random\n",
        "  import math\n",
        "\n",
        "  while(True):\n",
        "      n_count = 0\n",
        "      if n_count == 0:\n",
        "          #print(n_count)\n",
        "          xs, ys = datagenerator(data_dir)\n",
        "          assert len(xs)%args.batch_size ==0, \\\n",
        "          log('make sure the last iteration has a full batchsize, this is important if you use batch normalization!')\n",
        "\n",
        "\t  # Normalized the data sets\n",
        "          xs = xs.astype('float32')/255.0   # if the images are not normalized then divide by 255\n",
        "          ys = ys.astype('float32')/255.0   # if the images are not normalized then divide by 255\n",
        "\n",
        "          indices = list(range(xs.shape[0]))\n",
        "          n_count = 1\n",
        "      for _ in range(epoch_num):\n",
        "          np.random.shuffle(indices)    # shuffle\n",
        "          for i in range(0, len(indices), batch_size):\n",
        "              batch_x = xs[indices[i:i+batch_size]]\n",
        "              batch_y = ys[indices[i:i+batch_size]]\n",
        "              \n",
        "            #  random.seed(1)\n",
        "              #noise =  np.random.normal(0, args.sigma/255.0, batch_x.shape)\n",
        "              #batch_y = batch_x + noise \n",
        "              yield batch_y, batch_x\n",
        "\n",
        "\n",
        "##--------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "##--------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "patch_size, stride = 40, 10\n",
        "aug_times = 1\n",
        "num_noise_realiza = 2\n",
        "scales = [1] #, 0.9, 0.8, 0.7\n",
        "batch_size = args.batch_size\n",
        "\n",
        "\n",
        "\n",
        "def data_aug(img, mode=0):\n",
        "\n",
        "    if mode == 0:\n",
        "        return img\n",
        "    elif mode == 1:\n",
        "        return np.flipud(img)\n",
        "    elif mode == 2:\n",
        "        return np.rot90(img)\n",
        "    elif mode == 3:\n",
        "        return np.flipud(np.rot90(img))\n",
        "    elif mode == 4:\n",
        "        return np.rot90(img, k=2)\n",
        "    elif mode == 5:\n",
        "        return np.flipud(np.rot90(img, k=2))\n",
        "    elif mode == 6:\n",
        "        return np.rot90(img, k=3)\n",
        "    elif mode == 7:\n",
        "        return np.flipud(np.rot90(img, k=3))\n",
        "\n",
        "\n",
        "\n",
        "def noise_aug(mode=0):\n",
        "    # noise level augmentation\n",
        "    if mode == 0:\n",
        "         sigma = 2\n",
        "    elif mode == 1:\n",
        "         sigma = 4\n",
        "    elif mode == 2:\n",
        "         sigma = 6\n",
        "    elif mode == 3:\n",
        "         sigma = 8\n",
        "    elif mode == 4:\n",
        "         sigma = 10\n",
        "    elif mode == 5:\n",
        "         sigma = 12\n",
        "    elif mode == 6:\n",
        "         sigma = 14\n",
        "    elif mode == 7:\n",
        "         sigma = 16\n",
        "    elif mode == 8:\n",
        "         sigma = 18\n",
        "    elif mode == 9:\n",
        "         sigma = 20\n",
        "\n",
        "    return sigma\n",
        "\n",
        "\n",
        "def gen_patches(file_name, sigma):\n",
        "\n",
        "    # read image\n",
        "    clean_img = cv2.imread(file_name, 0)  # load from .png file\n",
        "    # I = loadmat(file_name)         \t  # load from .mat file\n",
        "    # clean_img = I['img']            \t  # load from .mat file\n",
        "    h, w = clean_img.shape\n",
        "\n",
        "    noise_sigma = sigma\n",
        "    noise = np.random.normal(0, noise_sigma, clean_img.shape)\n",
        "    img = clean_img + noise\n",
        "    \n",
        "    # showing image  \n",
        "    #fig = plt.figure(figsize=(12, 4))\n",
        "    #plt.subplot(1, 2, 1)\n",
        "    #plt.imshow(clean_img, cmap='gray')\n",
        "    #plt.colorbar()\n",
        "    #plt.title(\"Ground Truth\")\n",
        "        \n",
        "    #plt.subplot(1, 2, 2)\n",
        "    #plt.imshow(img, cmap='gray')\n",
        "    #plt.colorbar()\n",
        "    #plt.title(\"Noisy image\")\n",
        "\n",
        "\n",
        "    patches = []\n",
        "    clean_patches = []\n",
        "\n",
        "    for s in scales:\n",
        "        h_scaled, w_scaled = int(h*s), int(w*s)\n",
        "        img_scaled = img \n",
        "        clean_img_scaled = clean_img\n",
        "        #img_scaled = cv2.resize(img, (h_scaled,w_scaled), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        # extract patches\n",
        "        for i in range(0, h_scaled-patch_size+1, stride):\n",
        "            for j in range(0, w_scaled-patch_size+1, stride):\n",
        "                    x = img_scaled[i:i+patch_size, j:j+patch_size]\n",
        "                    clean_x = clean_img_scaled[i:i+patch_size, j:j+patch_size]\n",
        "\n",
        "                    #patches.append(x)        \n",
        "                    # data augmentation with different rotation\n",
        "                    for k in range(0, aug_times):\n",
        "                        mode_k=np.random.randint(0,8)\n",
        "                        x_aug = data_aug(x, mode=mode_k)                        \n",
        "                        clean_x_aug = data_aug(clean_x, mode=mode_k)\n",
        "                        \n",
        "                        patches.append(x_aug)\n",
        "                        clean_patches.append(clean_x_aug)\n",
        "                \n",
        "    return clean_patches, patches\n",
        "\n",
        "\n",
        "def datagenerator(data_dir='data/train5',verbose=False):\n",
        "    \n",
        "    file_list = glob.glob(data_dir+'/*.png')     # get name list of all .png files\n",
        "    # file_list = glob.glob(data_dir+'/*.mat')   # get name list of all .mat files\n",
        "\n",
        "    # initrialize\n",
        "    data = []\n",
        "    data_clean = []\n",
        "\n",
        "    # generate patches\n",
        "    for i in range(len(file_list)):\n",
        "\n",
        "        # Get a noise level\n",
        "        sigma = args.sigma \t\t\t\t  # For a fixed noise level\n",
        "\tsigma = noise_aug(mode=np.random.randint(0, 6))   # For a range of different noise level\n",
        "\n",
        "        #print(\"Noise level:\",sigma)\n",
        "\n",
        "        # data augmentation with different noise realization\n",
        "        for repeat_aug in range(0, num_noise_realiza):\n",
        "           \n",
        "        \tclean_patch, patch = gen_patches(file_list[i], sigma)\n",
        "\n",
        "        \tdata.append(patch)\n",
        "        \tdata_clean.append(clean_patch)\n",
        "        \n",
        "        #if verbose:\n",
        "        #   print(str(i+1)+'/'+ str(len(file_list)) + ' is done ^_^')\n",
        "\n",
        "\n",
        "    # do for noisy data\n",
        "    data = np.array(data, dtype='float32')\n",
        "    data = data.reshape((data.shape[0]*data.shape[1],data.shape[2],data.shape[3],1))\n",
        "    discard_n = len(data)-len(data)//batch_size*batch_size;\n",
        "    data = np.delete(data,range(discard_n),axis = 0)\n",
        "\n",
        "    # do for clean data\n",
        "    data_clean = np.array(data_clean, dtype='float32')\n",
        "    data_clean = data_clean.reshape((data_clean.shape[0]*data_clean.shape[1],data_clean.shape[2],data_clean.shape[3],1))\n",
        "    discard_n = len(data_clean)-len(data_clean)//batch_size*batch_size;\n",
        "    data_clean = np.delete(data_clean,range(discard_n),axis = 0)\n",
        "    print('^_^-training data finished-^_^')\n",
        "    print(data.shape)\n",
        "    #print(data_clean.shape)\n",
        "    \n",
        "    return data_clean, data\n",
        "\n",
        "\n",
        "##---------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# define loss\n",
        "def sum_squared_error(y_true, y_pred):\n",
        "    #return K.mean(K.square(y_pred - y_true), axis=-1)\n",
        "    #return K.sum(K.square(y_pred - y_true), axis=-1)/2\n",
        "    return K.sum(K.square(y_pred - y_true))/2\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # model selection\n",
        "    # Change the model depth based on the noise level or objective\n",
        "    model = DIVA2D(depth=10,filters=64,image_channels=1,use_bnorm=True)\n",
        "    model.summary()\n",
        "\n",
        "    # load the last model in matconvnet style\n",
        "    initial_epoch = findLastCheckpoint(save_dir=save_dir)\n",
        "    if initial_epoch > 0:  \n",
        "        print('resuming by loading epoch %03d'%initial_epoch)\n",
        "        #model = DIVA2D(depth=10,filters=64,image_channels=1,use_bnorm=True)\n",
        "        model = load_model(os.path.join(save_dir,'model_%03d.hdf5'%initial_epoch), compile=False)\n",
        "        #model.load_weights(os.path.join(save_dir,'model_%03d.hdf5'%initial_epoch))\n",
        "        log('load trained model architecture')\n",
        "    \n",
        "    # compile the model\n",
        "    model.compile(optimizer=Adam(0.001), \n",
        "                  loss= tf.keras.losses.MeanSquaredError(), #tf.keras.losses.CosineSimilarity (axis=-1, reduction=\"auto\", name=\"cosine_similarity\"), \n",
        "                  metrics=[tf.keras.metrics.MeanSquaredError(), \n",
        "                           tf.keras.metrics.RootMeanSquaredError(),\n",
        "                           tf.keras.metrics.MeanSquaredLogarithmicError(), \n",
        "                           tf.keras.metrics.MeanAbsoluteError(),\n",
        "                           sum_squared_error])\n",
        "\n",
        "    # tf.keras.metrics.MeanAbsolutePercentageError(), tf.keras.metrics.CosineSimilarity(name=\"cosine_similarity\", dtype=None, axis=-1), \n",
        "    # tf.keras.metrics.LogCoshError(), \n",
        "    \n",
        "    # use call back functions\n",
        "    checkpointer = ModelCheckpoint(os.path.join(save_dir,'model_{epoch:03d}.hdf5'), \n",
        "                verbose=1, save_weights_only=False, period=10)\n",
        "    csv_logger = CSVLogger(os.path.join(save_dir,'log.csv'), append=True, separator=',')\n",
        "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "    \n",
        "    history = model.fit_generator(train_datagen(batch_size=args.batch_size),\n",
        "                steps_per_epoch=1000, epochs=args.epoch, verbose=1, initial_epoch=initial_epoch,\n",
        "                callbacks=[checkpointer,csv_logger,lr_scheduler])\n",
        "    \n",
        "\n",
        "##---------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # plot loss functions\n",
        "    # list all data in history\n",
        "    print(history.history.keys())\n",
        "    # summarize history for loss\n",
        "    plt.plot((history.history['loss']))\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train'], loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}
