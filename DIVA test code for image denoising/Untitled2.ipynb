{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQKUdedwJl86"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "from argparse import ArgumentParser\n",
        "import glob\n",
        "import cv2\n",
        "import re\n",
        "import os, glob, datetime\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "from keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract, Reshape\n",
        "from keras.models import Model, load_model\n",
        "from tensorflow.python.keras.utils import conv_utils\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "\n",
        "from skimage.transform import rescale\n",
        "from scipy.io import loadmat\n",
        "import scipy.io as sio\n",
        "from scipy.io import savemat\n",
        "from matplotlib import pyplot as plt\n",
        "import skimage\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.io import imread, imsave\n",
        "\n",
        "#from skimage.measure import compare_psnr, compare_ssim\n",
        "\n",
        "\n",
        "\n",
        "#########--------------           IMPORTANT NOTE       ----------------###############\n",
        "\n",
        "# Select the path of the 'set_dir', 'set_names', 'model_dir', 'model_name' and 'sigma' properly to give access to the models, datasets and results\n",
        "\n",
        "# Check whether the training images are normalized or not.\n",
        "\n",
        "# Select the path of the directory properly to give access to the models, datasets and results\n",
        "directory_path =  '_________________________' #'Choose your directory\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Keras DIVA2D test')\n",
        "# choose if needed\n",
        "parser.add_argument('--model', default='DIVA2D', type=str, help='choose a type of model')\n",
        "parser.add_argument('--kernel_size', default=5, type=int, help='kernel size')\n",
        "\n",
        "parser.add_argument('--set_dir', default=os.path.join(directory_path ,'data/'), type=str, help='directory of test dataset')\n",
        "parser.add_argument('--set_names', default=['Set12'], type=list, help='name of test dataset')\n",
        "\n",
        "parser.add_argument('--sigma', default=25, type=int, help='noise level - Choose a sigma value from 10, 15, 25, 50, 75, 100')\n",
        "parser.add_argument('--model_dir', default=os.path.join(directory_path ,'models/DIVA_models_sigma_10_to_100'), type=str, help='directory of the model')\n",
        "                                                     \n",
        "parser.add_argument('--model_name', default='model_sigma25.hdf5', type=str, help='the model name')\n",
        "#parser.add_argument('--result_dir', default=os.path.join(directory_path,'results'), type=str, help='directory of results')\n",
        "parser.add_argument('--save_result', default=1, type=int, help='save the denoised image, 1 or 0')\n",
        "\n",
        "parser.add_argument('-f', '--file', required=False)\n",
        "args = parser.parse_args()\n",
        "\n",
        "args.result_dir = os.path.join(args.model_dir,'results')\n",
        "\n",
        "\n",
        "\n",
        "#########--------------           IMPORTANT NOTE       ----------------###############\n",
        "\n",
        "# for using pretrained model with sigma = 10, 15, 25, 50, 75, 100\n",
        "# Please modify the 'args.sigma' by the respective sigma vamue\n",
        "# Also change the 'args.model_dir' and the 'args.model_name' as defined below.\n",
        "\n",
        "#args.sigma = 15 # '_____' #'Choose a sigma value from 10, 15, 25, 50, 75, 100'\n",
        "args.model_dir = directory_path +'/models/DIVA_models_sigma_10_to_100'\n",
        "model = 'model_sigma'+str(args.sigma)+'.hdf5'\n",
        "args.model_name = model\n",
        "\n",
        "print(args.sigma)\n",
        "print(args.model_name)\n",
        "\n",
        "# Also set 'use_model = False / True ' (which load pretrained models)\n",
        "use_model = True \n",
        "\n",
        "\n",
        "##--------------------------------------------------------------------------------------------------------\n",
        "##--------------------------------------------------------------------------------------------------------\n",
        "\n",
        "class Hamiltonian_Conv2D(Conv2D):\n",
        "\n",
        "    def __init__(self, filters, kernel_size, kernel_3=None, kernel_4=None, activation=None, **kwargs):\n",
        "\n",
        "        self.rank = 2               # Dimension of the kernel\n",
        "        self.num_filters = filters  # Number of filter in the convolution layer\n",
        "        self.kernel_size = conv_utils.normalize_tuple(kernel_size, self.rank, 'kernel_size')\n",
        "        self.kernel_3 = kernel_3    # Weights from original potential\n",
        "        self.kernel_4 = kernel_4    # Weights from interaction     \n",
        "\n",
        "        super(Hamiltonian_Conv2D, self).__init__(self.num_filters, self.kernel_size, \n",
        "              activation=activation, use_bias=False, **kwargs)\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "        if input_shape[channel_axis] is None:\n",
        "            raise ValueError('The channel dimension of the inputs '\n",
        "                     'should be defined. Found `None`.')\n",
        "\n",
        "        #don't use bias:\n",
        "        self.bias = None\n",
        "\n",
        "        #consider the layer built\n",
        "        self.built = True\n",
        "\n",
        "\n",
        "        # Define nabla operator\n",
        "        weights_1 = tf.constant([[ 2.,-1., 0.],\n",
        "                                 [-1., 4.,-1.],\n",
        "                                 [ 0.,-1., 2.]])\n",
        "        \n",
        "\n",
        "        weights_1 = tf.reshape(weights_1 , [3,3, 1])\n",
        "        weights_1 = tf.repeat(weights_1 , repeats=self.num_filters, axis=2)\n",
        "        #print('kernel shape of weights_1:',weights_1.get_shape())\n",
        "\n",
        "        # Define Weights for h^2/2m  (size should be same as the nabla operator)\n",
        "        weights_2 = self.add_weight(shape=weights_1.get_shape(),\n",
        "                                      initializer= 'Orthogonal',\n",
        "                                      name='kernel_h^2/2m',\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        #print('kernel shape of weights_2:',weights_2.get_shape())\n",
        "\n",
        "        \n",
        "        # Define the Hamiltonian kernel\n",
        "        self.kernel = weights_1*weights_2 + self.kernel_3 + self.kernel_4\n",
        "        #print('self.kernel',self.kernel.get_shape())\n",
        "\n",
        "        self.built = True\n",
        "        super(Hamiltonian_Conv2D, self).build(input_shape)\n",
        "\n",
        "    # Do the 2D convolution using the Hamiltonian kernel\n",
        "    def convolution_op(self, inputs, kernel):\n",
        "        if self.padding == \"causal\":\n",
        "            tf_padding = \"VALID\"  # Causal padding handled in `call`.\n",
        "        elif isinstance(self.padding, str):\n",
        "            tf_padding = self.padding.upper()\n",
        "        else:\n",
        "            tf_padding = self.padding\n",
        "\n",
        "\n",
        "        return tf.nn.convolution(\n",
        "            inputs,\n",
        "            kernel,\n",
        "            strides=list(self.strides),\n",
        "            padding=tf_padding,\n",
        "            dilations=list(self.dilation_rate),\n",
        "            data_format=self._tf_data_format,\n",
        "            name=self.__class__.__name__,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        outputs = self.convolution_op(inputs, self.kernel)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def DIVA2D(depth,filters=64,image_channels=1, kernel_size= args.kernel_size, use_bnorm=True):\n",
        "    layer_count = 0\n",
        "    inpt = Input(shape=(None,None,image_channels),name = 'input'+str(layer_count))\n",
        "    \n",
        "    # Get the initial patches /initial_patches\n",
        "    initial_patches = Conv2D(filters=filters, kernel_size=(kernel_size,kernel_size), strides=(1,1),kernel_initializer='Orthogonal', padding='same',name = 'initial_patches')(inpt)\n",
        "    initial_patches = Activation('relu',name = 'initial_patch_acti')(initial_patches)\n",
        "    #print(initial_patches.get_shape())\n",
        "\n",
        "    # interaction layer\n",
        "    inter = Conv2D(filters=filters, kernel_size=(kernel_size,kernel_size), strides=(1,1),kernel_initializer='Orthogonal', padding='same',name = 'interactions')(initial_patches)\n",
        "    inter = Activation('relu',name = 'interaction_acti'+str(layer_count))(inter)\n",
        "    #print(inter.get_shape())\n",
        "\n",
        "\n",
        "    # Get contributions of the original potential in the Hamiltonian kernel\n",
        "    ori_poten_kernel = tf.keras.layers.MaxPooling2D (pool_size=(21,21), strides=(15,15), padding='same', name = 'ori_poten_ker', data_format=None )(initial_patches)\n",
        "    #print('ori_poten_kernel',ori_poten_kernel.get_shape())\n",
        "\n",
        "    # Get contributions of the interactions in the Hamiltonian kernel\n",
        "    inter_kernel = tf.keras.layers.MaxPooling2D (pool_size=(21,21), strides=(15,15), padding='same', name = 'inter_ker', data_format=None )(inter)\n",
        "    #print('inter_kernel',inter_kernel.get_shape())\n",
        "\n",
        "\n",
        "    # Get projection coefficients of the initial patches on the Hamiltonian kernel\n",
        "    x = Hamiltonian_Conv2D(filters=filters, kernel_size=(kernel_size,kernel_size), kernel_3 = ori_poten_kernel, kernel_4 = inter_kernel, strides=(1,1), activation='relu',\n",
        "                              kernel_initializer='Orthogonal', padding='same', name = 'proj_coef')(initial_patches)      \n",
        "    \n",
        "    #print('coef',x.get_shape())\n",
        "\n",
        "\n",
        "    # Do Thresholding (depth depends on the noise intensity)\n",
        "    for i in range(depth-2):\n",
        "      layer_count += 1\n",
        "      x = Conv2D(filters=filters, kernel_size=(kernel_size,kernel_size), strides=(1,1),kernel_initializer='Orthogonal', padding='same',use_bias = False,name = 'conv'+str(layer_count))(x)\n",
        "\n",
        "      layer_count += 1\n",
        "      x = BatchNormalization(axis=3, momentum=0.1,epsilon=0.0001, name = 'bn'+str(layer_count))(x) \n",
        "        #x = BatchNormalization(axis=3, momentum=0.0,epsilon=0.0001, name = 'bn'+str(layer_count))(x)\n",
        "      \n",
        "      # Thresholding\n",
        "      x = Activation('relu',name = 'Thresholding'+str(layer_count))(x)  \n",
        "\n",
        "    # Inverse projection\n",
        "    x = Conv2D(filters=image_channels, kernel_size=(kernel_size,kernel_size), strides=(1,1), kernel_initializer='Orthogonal',padding='same',use_bias = False,name = 'inv_trans')(x)\n",
        "\n",
        "    x = Subtract(name = 'subtract')([inpt, x])\n",
        "\n",
        "    model = Model(inputs=inpt, outputs=x)\n",
        "    \n",
        "    return model\n",
        "\n",
        "##----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "def to_tensor(img):\n",
        "    if img.ndim == 2:\n",
        "        return img[np.newaxis,...,np.newaxis]\n",
        "    elif img.ndim == 3:\n",
        "        return np.moveaxis(img,2,0)[...,np.newaxis]\n",
        "\n",
        "\n",
        "def from_tensor(img):\n",
        "    return np.squeeze(np.moveaxis(img[...,0],0,-1))\n",
        "\n",
        "def log(*args,**kwargs):\n",
        "     print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S:\"),*args,**kwargs)\n",
        "\n",
        "def save_result(result,path):\n",
        "    path = path if path.find('.') != -1 else path+'.png'\n",
        "    ext = os.path.splitext(path)[-1]\n",
        "    if ext in ('.txt','.dlm'):\n",
        "        np.savetxt(path,result,fmt='%2.4f')\n",
        "    else:\n",
        "        imsave(path,np.clip(result,0,1))\n",
        "\n",
        "\n",
        "def show(x,title=None,cbar=False,figsize=None):\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(x,interpolation='nearest',cmap='gray')\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    if cbar:\n",
        "        plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "def psnr(target, ref):\n",
        "    # Assume target is RGB/BGR image\n",
        "    target_data = target.astype(np.float32)\n",
        "    ref_data = ref.astype(np.float32)\n",
        "    \n",
        "    diff = ref_data - target_data\n",
        "    diff = diff.flatten('C')\n",
        "    \n",
        "    rmse = np.sqrt(np.mean(diff ** 2.))\n",
        "    \n",
        "    return 20 * np.log10(1. / rmse)\n",
        "\n",
        "\n",
        "def snr(target, ref):\n",
        "    # Assume target is RGB/BGR image\n",
        "    target_data = target.astype(np.float32)\n",
        "    ref_data = ref.astype(np.float32)\n",
        "    \n",
        "    diff = ref_data - target_data\n",
        "    diff = diff.flatten('C')\n",
        "    target_data = target_data.flatten('C')\n",
        "    \n",
        "    rmse_diff = np.sqrt(np.mean(diff ** 2.))\n",
        "    rmse_target = np.sqrt(np.mean(target_data ** 2.))\n",
        "\n",
        "    return 20 * np.log10(rmse_target / rmse_diff)\n",
        "\n",
        "\n",
        "##----------------------------------------------------------------------------------------------------------------\n",
        "## -------------------------------------------------------------------------------------------------------------\n",
        "## -------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "if __name__ == '__main__':    \n",
        "    \n",
        "    if  use_model:\n",
        "        #choose model depth and filters number\n",
        "        model = DIVA2D(depth=10,filters=64,image_channels=1,use_bnorm=True)\n",
        "        model.load_weights(os.path.join(args.model_dir, args.model_name))\n",
        "        log('load trained model architecture')\n",
        "    else:\n",
        "        print('Model- ',args.model)\n",
        "        model = load_model(os.path.join(args.model_dir, args.model_name),compile=False)\n",
        "        log('load trained model')\n",
        "\n",
        "    if not os.path.exists(args.result_dir):\n",
        "        os.mkdir(args.result_dir)\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    for set_cur in args.set_names:  \n",
        "        \n",
        "        if not os.path.exists(os.path.join(args.result_dir,set_cur)):\n",
        "            os.mkdir(os.path.join(args.result_dir,set_cur))\n",
        "        snrs_in = []\n",
        "        psnrs_in = []\n",
        "        ssims_in = [] \n",
        "        psnrs = []\n",
        "        ssims = [] \n",
        "        \n",
        "        for im in os.listdir(os.path.join(args.set_dir,set_cur)): \n",
        "            if im.endswith(\".mat\") or im.endswith(\".jpg\") or im.endswith(\".bmp\") or im.endswith(\".png\"):\n",
        "\n",
        "                # I = loadmat(os.path.join(args.set_dir,set_cur,im))        # For .mat files\n",
        "                # I_0 = I['img']              \t\t\t\t    # For .mat files\n",
        "                # I_1 = (I_0 - np.min(I_0))\t\t\t   \t    # For .mat files\n",
        "                # x = (I_1/ np.max(I_1))   # normalized the image           # For .mat files\n",
        "\n",
        "\t\t            x = np.array(imread(os.path.join(args.set_dir,set_cur,im)), dtype=np.float32) / 255.0   # For .png files\n",
        "\n",
        "                \n",
        "                # Gaussian noise case\n",
        "                y = x + np.random.normal(0, args.sigma/255.0, x.shape) # Add Gaussian noise without clipping\n",
        "                y = y.astype(np.float32)\n",
        "\n",
        "                y_  = to_tensor(y)\n",
        "\n",
        "                start_time = time.time()\n",
        "                x_ = model.predict(y_) # inference\n",
        "\n",
        "\n",
        "                elapsed_time = time.time() - start_time\n",
        "                print('%10s : image:%10s : time:%2.4f second'%(set_cur,im,elapsed_time))\n",
        "\n",
        "                x_=from_tensor(x_)\n",
        "\n",
        "                # calculate for Gaussian noise\n",
        "                snr_y = snr(x, y)        # input SNR\n",
        "                psnr_y = psnr(x, y)      # input PSNR\n",
        "                ssim_y = ssim(x, y)      # input SSIM\n",
        "                psnr_x_ = psnr(x, x_)\t # output PSNR\n",
        "                ssim_x_ = ssim(x, x_)\t # output SSIM\n",
        "                print('%10s : psnr = %2.4f : ssim = %1.4f'%(set_cur,psnr_x_, ssim_x_))\n",
        "\n",
        "\n",
        "\n",
        "                if args.save_result:\n",
        "                    name, ext = os.path.splitext(im)\n",
        "\n",
        "                    # showing image  \n",
        "                    show(np.hstack((y,x_,x)),figsize=(14, 4),cbar=True) # show the image\n",
        "                   \n",
        "                    save_result(y,path=os.path.join(args.result_dir,set_cur,name+'_noisy_'+str(args.sigma)+ext)) # save the noisy image\n",
        "                    save_result(x_,path=os.path.join(args.result_dir,set_cur,name+'_denoised_DIVA_'+str(args.sigma)+ext)) # save the denoised image\n",
        "                    \n",
        "\t\t                # For .mat files\n",
        "\t\t                # img_data = {'img_noi' : y,    'img_denoi' : x_,}\n",
        "\t \t                # savemat(os.path.join(args.result_dir,set_cur,name+'_denoised_DIVA2D_sigma'+str(args.sigma)+'.mat'), img_data) # save the 2D images\n",
        "                   \n",
        "                snrs_in.append(snr_y)     # input SNR\n",
        "                psnrs_in.append(psnr_y)   # input PSNR\n",
        "                ssims_in.append(ssim_y)   # input SSIM\n",
        "                psnrs.append(psnr_x_)\t    # output PSNR\n",
        "                ssims.append(ssim_x_)\t    # output SSIM\n",
        "                \n",
        "\n",
        "        snr_in_avg = np.mean(snrs_in)     # average input SNR\n",
        "        psnr_in_avg = np.mean(psnrs_in)   # average input PSNR\n",
        "        ssim_in_avg = np.mean(ssims_in)   # average input SSIM\n",
        "        psnr_avg = np.mean(psnrs)\t        # average output PSNR\n",
        "        ssim_avg = np.mean(ssims)\t        # average output SSIM\n",
        "\n",
        "        snrs_in.append(snr_in_avg)      # input SNR\n",
        "        psnrs_in.append(psnr_in_avg)    # input PSNR\n",
        "        ssims_in.append(ssim_in_avg)    # input SSIM\n",
        "        psnrs.append(psnr_avg)\t\t      # output PSNR\n",
        "        ssims.append(ssim_avg)\t\t      # output SSIM\n",
        "        \n",
        "        if args.save_result:\n",
        "            save_result(np.hstack((psnrs, ssims)),path=os.path.join(args.result_dir,set_cur,'sigma_'+str(args.sigma)+'_results.txt'))\n",
        "            save_result(np.hstack((snrs_in, psnrs_in, ssims_in)),path=os.path.join(args.result_dir,set_cur,'sigma_'+str(args.sigma)+'_results_input.txt'))\n",
        "\n",
        "        log('Datset: {0:10s} \\n  SNR_in = {1:2.2f} dB, PSNR_in = {2:2.2f} dB, SSIM_in = {3:1.4f}'.format(set_cur, snr_in_avg, psnr_in_avg, ssim_in_avg))\n",
        "        log('PSNR = {0:2.2f} dB, SSIM = {1:1.4f}'.format(psnr_avg, ssim_avg))\n",
        "\n"
      ]
    }
  ]
}